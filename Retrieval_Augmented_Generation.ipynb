{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log in with HF access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113332,
     "status": "ok",
     "timestamp": 1745385505876,
     "user": {
      "displayName": "Nadejda Stekolnikova",
      "userId": "12364186662498579866"
     },
     "user_tz": -300
    },
    "id": "ZQ-SbgkB76uO",
    "outputId": "cbf5d924-2d33-4103-8374-396d1e1eeb8c"
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the knowledge base and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXfa0Wpu8Bmv"
   },
   "outputs": [],
   "source": [
    "import os, pickle, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Pre-built Index and Knowledge Base\n",
    "print(\"Loading pre-built FAISS index and knowledge base...\")\n",
    "index = faiss.read_index(\"index.idx\")\n",
    "with open(\"knowledge_base.pkl\", \"rb\") as f:\n",
    "    knowledge_base = pickle.load(f)\n",
    "\n",
    "# Load a pre-trained Sentence Transformer model (multilingual)\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6mJdVaY8dVk"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(query, index, knowledge_base, embedding_model, top_k=3):\n",
    "    \"\"\"Retrieves the top_k most relevant documents.\"\"\"\n",
    "    index.nprobe = 10 # Search in 10 closest clusters\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [knowledge_base[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1745386324049,
     "user": {
      "displayName": "Nadejda Stekolnikova",
      "userId": "12364186662498579866"
     },
     "user_tz": -300
    },
    "id": "U96mAw408zYy",
    "outputId": "f6e3db22-2324-41f3-fe24-7052e706083e"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-1.1-7b-it\", # or model=\"google/gemma-2-9b-it\" (newer and more powerful)\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def generate_answer_with_context(query, retrieved_documents, generator):\n",
    "    \"\"\"Generates an answer based on the query and retrieved documents.\"\"\"\n",
    "    context = \"\\n\\n\".join(retrieved_documents)\n",
    "    prompt = f\"Based on the following context, answer the question. \\nContext: {context}\\nQuestion: {query}\"\n",
    "    output = generator(prompt, max_new_tokens=200, num_return_sequences=1)\n",
    "    answer = output[0]['generated_text'].split(\"Answer:\")[1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43544,
     "status": "ok",
     "timestamp": 1745386554389,
     "user": {
      "displayName": "Nadejda Stekolnikova",
      "userId": "12364186662498579866"
     },
     "user_tz": -300
    },
    "id": "lX7lnwHG86z2",
    "outputId": "11c8a4f2-5447-45ed-f86f-723f98e6ff50"
   },
   "outputs": [],
   "source": [
    "query = \"\"  # Question to answer\n",
    "retrieved_docs = retrieve_relevant_documents(query, index, knowledge_base, embedding_model)\n",
    "print(\"Retrieved Document(s):\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{i}- {doc[:128]}...\")\n",
    "\n",
    "answer = generate_answer_with_context(query, retrieved_docs, generator)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
